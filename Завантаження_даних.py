# -*- coding: utf-8 -*-
"""Завантаження даних "Мережка аналіз асортименту.ipynb"

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YYjBSlhYriD6t7LYllAfKklnsOPrlPag
"""

import requests, time, re
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import pandas as pd
from collections import OrderedDict
import os

# Завантаження списку категорій

# 1. Збираємо дані каталогу
#
# Створюємо список усіх можливих сторінок каталогу
# Нумерація сторінок починається з 0
# Посилання є специфічним для цього сайту
base_url = 'https://merezhka.com.ua/catalog/all?sort=name%7CASC&page='
list_all_url = []
for i in range(0, 556):
    list_all_url.append(base_url + str(i))

# Створюємо запит на збирання посилань на товари каталогу

session = requests.Session()

urls_all_product_list = []
for i in list_all_url:
    r = session.get(i)
    p = BeautifulSoup(r.text, "html.parser")

    # Занурюємося у список де лежать товари
    view_content = p.find(class_="view-content")

    # Дістаємо посилання на товари
    titles = view_content.find_all(class_="b-product-small__title")
    urls = []
    for title in titles:
        a_tag = title.find("a", href=True)
        if a_tag:
            urls.append(a_tag["href"])

    # Доповнюємо загальний список
    urls_all_product_list.append(urls)
    del urls
    print("Done urls: ", i)
    time.sleep(1)

# Цикл утворив список списків. Зробимо один єдиний список товарів
urls_all_product_list_all = [item for sublist in urls_all_product_list for item in sublist]
# Зберігаємо перелік товарів
pd.to_pickle(urls_all_product_list_all, "urls_all_product_list.pkl")

from google.colab import files

pd.to_pickle(urls_all_product_list_all, "urls_all_product_list.pkl")
files.download("urls_all_product_list.pkl")

from joblib import Parallel, delayed

def load_data_from_merezhka(path_urls: str) -> pd.DataFrame:
    session = requests.Session()

    try:
        r = session.get(path_urls)
        r.raise_for_status()
    except Exception as e:
        session.close()
        empty = pd.DataFrame(columns=["name_attr","value_attr","price","urls","goods_name","goods_category","goods_status","goods_code"])
        print(f"[ERROR GET] {path_urls} -> {e}")
        return empty

    p = BeautifulSoup(r.text, "html.parser")

    # Назва
    title_tag = p.find(class_="b-product__title")
    goods_name = title_tag.get_text(strip=True) if title_tag else 'no_data'

    # Категорія
    category_block = p.find(class_="b-product__topline_bottom")
    if category_block:
        field_item = category_block.find(class_="field__item")
        goods_category = field_item.get_text(strip=True) if field_item else 'no_data'
    else:
        goods_category = 'no_data'

    # Статус
    container = p.find("div", class_="b-product__topline_bottom")
    if container:
        all_classes = set(container.get("class", []))
        for tag in container.find_all(True):
            all_classes.update(tag.get("class", []))
        if {"product-avail", "on"}.issubset(all_classes):
            status_tag = container.find(class_="product-avail on")
            goods_status = status_tag.get_text(strip=True) if status_tag else 'no_data'
        elif {"product-avail", "off"}.issubset(all_classes):
            status_tag = container.find(class_="product-avail off")
            goods_status = status_tag.get_text(strip=True) if status_tag else 'no_data'
        else:
            goods_status = 'no_data'
    else:
        goods_status = 'no_data'

    # Ціна
    price_tag = p.find(class_='b-price')
    goods_price = price_tag.get_text(strip=True) if price_tag else 'no_data'

    # Код товару
    code_block = p.find("div", class_=lambda x: x and "field--name-field-product-vendor-code" in x)
    if code_block:
        code_item = code_block.find(class_="field__item")
        goods_code = code_item.get_text(strip=True) if code_item else 'no_data'
    else:
        goods_code = 'no_data'


    # Перший блок характеристик
    attr_block = p.find(
        class_='field field--name-field-product-specifications field--type-vocabilary-terms-field field--label-hidden field__items'
    )
    if attr_block:
        name_attr_1 = [x.get_text(strip=True) for x in attr_block.find_all(class_='label')]
        value_attr_1 = [x.get_text(strip=True) for x in attr_block.find_all(class_='name')]
    else:
        name_attr_1, value_attr_1 = [], []

    # Другий блок характеристик
    container2 = p.find(class_='container-inline field__item')
    if container2:
        name_attr_2 = [x.get_text(strip=True) for x in container2.find_all(class_='double-field-first')]
        value_attr_2 = [x.get_text(strip=True) for x in container2.find_all(class_='double-field-second')]
    else:
        name_attr_2, value_attr_2 = [], []

    # Об’єднання
    name_all = name_attr_1 + name_attr_2
    value_all = value_attr_1 + value_attr_2
    max_len = max(len(name_all), len(value_all), 1)
    while len(name_all) < max_len:
        name_all.append('no_data')
    while len(value_all) < max_len:
        value_all.append('no_data')

    goods_attr_df = pd.DataFrame({'name_attr': name_all, 'value_attr': value_all})
    goods_attr_df['price'] = goods_price
    goods_attr_df['urls'] = path_urls
    goods_attr_df['goods_name'] = goods_name
    goods_attr_df['goods_category'] = goods_category
    goods_attr_df['goods_status'] = goods_status
    goods_attr_df['goods_code'] = goods_code


    col_order = [
    "goods_name",
    "goods_category",
    "goods_code",
    "goods_status",
    "price",
    "urls",
    "name_attr",
    "value_attr"
    ]
    goods_attr_df = goods_attr_df[col_order]

    session.close()
    return goods_attr_df

import requests, time, re
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import pandas as pd
from collections import OrderedDict
import os
from google.colab import files
print("Завантаж файл urls_all_product_list.pkl")
files.upload()

# 3. Читаємо список URL-ів товарів і формуємо абсолютні посилання
urls_all_product_list_all = pd.read_pickle("urls_all_product_list.pkl")
list_all_url_goods = ['https://merezhka.com.ua' + u for u in urls_all_product_list_all]

# 4. Підхоплюємо попередні результати
import os

if os.path.exists("goods_info_all_df.pkl"):
    goods_info_all_df = pd.read_pickle("goods_info_all_df.pkl")
elif os.path.exists("goods_info_all_df.csv"):
    goods_info_all_df = pd.read_csv("goods_info_all_df.csv")
else:
    goods_info_all_df = pd.DataFrame(columns=[
        "name_attr","value_attr","price","urls","goods_name","goods_category","goods_status", "goods_code"
    ])

processed_urls = set(goods_info_all_df['urls'].unique()) if not goods_info_all_df.empty else set()

print(f"Всього товарів: {len(list_all_url_goods)}")
print(f"Вже оброблено: {len(processed_urls)}")
print(f"Залишилось: {len(list_all_url_goods) - len(processed_urls)}")

# Якщо хочеш пропустити вже оброблені урли:
urls_for_run = [u for u in list_all_url_goods if u not in processed_urls]
#list_all_url_goods
 #

results_list = Parallel(n_jobs=-1, verbose=10, prefer="threads")(
    delayed(load_data_from_merezhka)(path_urls=u) for u in urls_for_run
)

results_df_all = pd.concat(results_list, ignore_index=True) if results_list else pd.DataFrame(
    columns = ["goods_code", "goods_name", "goods_category", "goods_status", "price", "urls", "name_attr", "value_attr"]

)

# Додаємо до вже наявних (якщо були)
goods_info_all_df = pd.concat([goods_info_all_df, results_df_all], ignore_index=True)

goods_info_all_df = goods_info_all_df[[c for c in col_order if c in goods_info_all_df.columns]]
goods_info_all_df.to_pickle("goods_info_all_df.pkl")
goods_info_all_df.to_csv("goods_info_all_df.csv", index=False)

files.download("goods_info_all_df.pkl")

df = pd.read_picklegoods_info_all_df

# . ТУТ ДОВАНТАЖЕННЯ, 70 штук тканин яких пропустило 16.09.2025 р. Якщо хочеш пропустити вже оброблені урли:

import pandas as pd
from joblib import Parallel, delayed
from google.colab import files
import pickle


# === 1. Завантажуємо файл з Colab ===
uploaded = files.upload()

file_name = list(uploaded.keys())[0]  # беремо ім'я файлу
print(f"[INFO] Файл завантажено: {file_name}")

# === 2. Читаємо як pickle ===
with open(file_name, "rb") as f:
    obj = pickle.load(f)

# Якщо pickle містить dict, дістаємо DataFrame
if isinstance(obj, dict):
    print("[DEBUG] Файл містить dict з ключами:", obj.keys())
    # вибери правильний ключ (наприклад "data" або "df")
    goods_info_all_df = obj[list(obj.keys())[0]]
else:
    goods_info_all_df = obj

print(f"[INFO] Завантажено DataFrame, рядків: {len(goods_info_all_df)}")

# === 3. Оброблені урли ===
processed_urls = set(goods_info_all_df['urls'].unique()) if not goods_info_all_df.empty else set()

print(f"Всього товарів: {len(list_all_url_goods)}")
print(f"Вже оброблено: {len(processed_urls)}")
print(f"Залишилось: {len(list_all_url_goods) - len(processed_urls)}")

# === 4. Список тих, які ще не оброблені ===
urls_for_run = [u for u in list_all_url_goods if u not in processed_urls]

# === 5. Парсимо тільки нові ===
results_list = Parallel(n_jobs=-1, verbose=10, prefer="threads")(
    delayed(load_data_from_merezhka)(path_urls=u) for u in urls_for_run
)

results_df_all = pd.concat(results_list, ignore_index=True) if results_list else pd.DataFrame(
    columns=["goods_code", "goods_name", "goods_category", "goods_status",
             "price", "urls", "name_attr", "value_attr"]
)

# === 6. Додаємо нові дані до існуючого ===
if not results_df_all.empty:
    goods_info_all_df = pd.concat([goods_info_all_df, results_df_all], ignore_index=True)

# === 7. Зберігаємо назад ===
goods_info_all_df.to_pickle("goods_info_all_df.pkl")
goods_info_all_df.to_csv("goods_info_all_df.csv", index=False)

print(f"[INFO] Оновлено goods_info_all_df.pkl, тепер рядків: {len(goods_info_all_df)}")

files.download("goods_info_all_df.pkl")

# товари з >1 категорією
multi_cat_products = product_categories[product_categories['unique_categories'] > 1]

print("Загальна кількість товарів:", product_categories.shape[0])
print("Товарів з >1 категорією:", multi_cat_products.shape[0])

# показати приклади
for _, row in multi_cat_products.iterrows():
    print(f"Товар: {row['product']}")
    print("Категорії:", ", ".join(row['categories']))
    print("-" * 80)

# 4. Повні дублікати по всіх колонках
full_dupes = data.duplicated(keep=False)
print("Повних дублікатів:", full_dupes.sum())

import pandas as pd

# 1. Завантажуємо файл з URL-ами
uploaded = files.upload()  # обираєш urls_all_product_list.pkl
urls_all_product_list = pd.read_pickle(list(uploaded.keys())[0])

# 2. Створюємо повні URL-и
list_all_url_goods = ['https://merezhka.com.ua' + u for u in urls_all_product_list]

# 3. Перевірка на унікальність
total_urls = len(list_all_url_goods)
unique_urls = len(set(list_all_url_goods))
dupes_count = total_urls - unique_urls

print("Всього URL:", total_urls)
print("Унікальних URL:", unique_urls)
print("Дублікатів URL:", dupes_count)

import pandas as pd

# Припустимо, що urls_all_product_list_all вже завантажений
url_counts = pd.Series(urls_all_product_list).value_counts()

# Виведемо тільки дублікати
duplicates = url_counts[url_counts > 1]
print("Повторювані URL і кількість їх входжень:")
print(duplicates)

import pickle
import pandas as pd
from google.colab import files

# === 1. Завантажуємо обидва файли ===
uploaded = files.upload()  # вибери одразу 2 файли
print("Файли завантажено:", uploaded.keys())

# === 2. Функція для читання будь-якого формату ===
def load_file(fname):
    if fname.endswith(".pkl"):
        return pd.read_pickle(fname)
    elif fname.endswith(".csv"):
        return pd.read_csv(fname)
    else:
        raise ValueError(f"Невідомий формат: {fname}")

# === 3. Читаємо обидва файли в DataFrame ===
file_names = list(uploaded.keys())
df1 = load_file(file_names[0])
df2 = load_file(file_names[1])

print(f"[INFO] df1 рядків: {len(df1)}, колонок: {df1.shape[1]}")
print(f"[INFO] df2 рядків: {len(df2)}, колонок: {df2.shape[1]}")

# === 4. Порівняння ===
# різниця за кількістю рядків
print("Різниця рядків:", len(df1) - len(df2))

# які унікальні урли є тільки в df1 і df2
urls1 = set(df1["urls"].unique())
urls2 = set(df2["urls"].unique())

only_in_df1 = urls1 - urls2
only_in_df2 = urls2 - urls1

print(f"Унікальні url тільки в df1: {len(only_in_df1)}")
print(f"Унікальні url тільки в df2: {len(only_in_df2)}")

# === 5. Якщо треба – зберігаємо різниці у файли ===
pd.DataFrame(sorted(list(only_in_df1)), columns=["urls"]).to_csv("only_in_df1.csv", index=False)
pd.DataFrame(sorted(list(only_in_df2)), columns=["urls"]).to_csv("only_in_df2.csv", index=False)

import pandas as pd
import numpy as np
import re
from typing import Dict, Callable

import pandas as pd
import numpy as np
import re
from typing import Callable, Dict

# --- Допоміжні екстрактори ---
def extract_article(name: str):
    if pd.isna(name): return None
    s = str(name)
    m = re.search(r'\b(?:арт\.?|арт|art|sku|код)[:\s\-]*([A-Za-zА-Яа-я0-9\-_/]+)\b', s, flags=re.I)
    if m: return m.group(1).strip()
    m2 = re.search(r'\b([A-Za-zА-Яа-я]{1,3}\d{2,6}|\d{3,6}(/[0-9]+)?)\b', s)
    if m2: return m2.group(1).strip()
    return None

def extract_length(name: str):
    if pd.isna(name): return None
    s = str(name)
    m = re.search(r'(\d+[.,]?\d*\s*(м|см|мм))\b', s, flags=re.I)
    if m:
        return m.group(1).replace('.',',').strip()
    return None

def extract_size(name: str):
    if pd.isna(name): return None
    s = str(name)
    m = re.search(r'(\d+[.,]?\d*\s*[\*xхX]\s*\d+[.,]?\d*\s*см\.?)', s)
    if m:
        val = m.group(1)
        val = re.sub(r'\s*[\*xхX]\s*', 'x', val)
        val = val.replace('.', ',').replace('см.', 'см')
        return val.strip()
    m2 = re.search(r'(\d+[.,]?\d*\s*см\.?)', s)
    if m2:
        val = m2.group(1).replace('.', ',').replace('см.', 'см')
        return val.strip()
    return None

def extract_color(name: str, known_colors: set = None):
    if pd.isna(name): return None
    s = str(name)
    if known_colors:
        for c in known_colors:
            if c.lower() in s.lower():
                return c
    m = re.search(r'\b[Кк]олір[:\s\-]*([A-Яа-яA-Za-z0-9 ,\-]+)', s)
    if m: return m.group(1).strip()
    parts = re.findall(r'\b([A-Za-zА-Яа-я\-]{3,20}(?:\s+[A-Za-zА-Яа-я\-]{3,20})?)\b', s)
    for p in parts[::-1]:
        if not re.search(r'\d', p) and len(p.split())<=3:
            return p.strip()
    return None

def extract_manufacturer(name: str, known_makers: set = None):
    if pd.isna(name): return None
    s = str(name)
    if known_makers:
        for m in known_makers:
            if m.lower() in s.lower():
                return m
    return None

# --- Словник категорій та атрибутів ---
per_subcat_extractors: Dict[str, Dict[str, Callable]] = {
    "Нитки для вишивання": {
        "Довжина": extract_length,
        "Колір": lambda s: extract_color(s, known_colors=set(df.loc[df['name_attr_clean']=='Колір','value_attr'].dropna().unique())),
        "Артикул": extract_article,
        "Виробник": lambda s: extract_manufacturer(s, known_makers=set(df.loc[df['name_attr_clean']=='Виробник','value_attr'].dropna().unique()))
    },
    "Схеми для вишивання бісером": {
        "Розмір": extract_size,
        "Тип": lambda s: None
    },
    "Набори для вишивання хрестиком": {
        "Розмір вишивки": extract_size,
        "Кількість": lambda s: None,
        "Артикул": extract_article
    },
    # Можна додати інші категорії за потребою
}

# --- Застосування екстракторів ---
log_rows = []
rows_to_add = []

# Беремо всі підкатегорії з value_attr == "Вид"
subcats = df.loc[df['name_attr_clean']=='Вид','value_attr'].dropna().unique().tolist()

for sub in subcats:
    if sub not in per_subcat_extractors:
        continue
    extractors = per_subcat_extractors[sub]
    dsub = df[df['value_attr']==sub]
    urls = dsub['urls'].unique()

    for attr, func in extractors.items():
        # Додаємо нові рядки для відсутніх атрибутів
        urls_with = set(df.loc[(df['value_attr']==sub) & (df['name_attr_clean']==attr) & (df['value_attr'].notna()), 'urls'].unique())
        urls_missing = [u for u in urls if u not in urls_with]
        for u in urls_missing:
            goods_name = df.loc[df['urls']==u, 'goods_name'].iloc[0]
            val = func(goods_name)
            if val is not None:
                rows_to_add.append({
                    'name_attr_clean': attr,
                    'name_attr': attr,
                    'value_attr': val,
                    'price': df.loc[df['urls']==u,'price'].iloc[0] if 'price' in df.columns else np.nan,
                    'currency': df.loc[df['urls']==u,'currency'].iloc[0] if 'currency' in df.columns else np.nan,
                    'urls': u,
                    'goods_name': goods_name,
                    'goods_category': sub,
                    'goods_status': df.loc[df['urls']==u,'goods_status'].iloc[0] if 'goods_status' in df.columns else np.nan
                })
                log_rows.append({
                    'urls': u,
                    'goods_name': goods_name,
                    'goods_category': sub,
                    'action': 'created_new',
                    'attribute': attr,
                    'old_value': None,
                    'new_value': val,
                    'source': 'goods_name'
                })

# Додаємо нові рядки
if rows_to_add:
    df_new = pd.DataFrame(rows_to_add)
    df = pd.concat([df, df_new], ignore_index=True)
    df = df.drop_duplicates(subset=['urls','name_attr_clean','value_attr']).reset_index(drop=True)

# Лог збережемо для рев'ю
log_df = pd.DataFrame(log_rows)

# Збереження проміжного результату
df.to_pickle("goods_info_after_subcat_extract.pkl")

print("Готово — екстракт з підкатегорій виконано. Лог збережено у goods_info_after_subcat_extract.pkl")

new_df = pd.read_pickle("goods_info_after_subcat_extract.pkl")
new_df.shape

import pandas as pd
from google.colab import files

# 1. Завантажуємо файл з усіма URL-ами
uploaded_urls = files.upload()  # вибираєш urls_all_product_list.pkl
urls_all_product_list_all = pd.read_pickle(list(uploaded_urls.keys())[0])
list_all_url_goods = ['https://merezhka.com.ua' + u for u in urls_all_product_list_all]

# 2. Завантажуємо DataFrame з інформацією про товари
uploaded_goods = files.upload()  # вибираєш goods_info_all_df.pkl
goods_info_all_df = pd.read_pickle(list(uploaded_goods.keys())[0])

# 3. Перевіряємо унікальні URL-и
print("Унікальних urls:", goods_info_all_df['urls'].nunique())

# 4. Повні дублікати по всіх колонках
full_dupes = goods_info_all_df.duplicated(keep=False)
print("Повних дублікатів:", full_dupes.sum())

# 5. Дублі по URL + атрибут + значення
subset_dupes = goods_info_all_df.duplicated(subset=['urls','name_attr','value_attr'], keep=False)
print("Дублі по url+атрибут+значення:", subset_dupes.sum())

# 6. Перегляд перших рядків дублікатів
dupes_df = goods_info_all_df[subset_dupes].sort_values(['urls','name_attr','value_attr'])

# 7. Перевірка, чи всі URL-и присутні у DataFrame
missing_urls = set(list_all_url_goods) - set(goods_info_all_df['urls'])
print("Відсутні URL-и у DataFrame:", len(missing_urls))

print("Відсутні URL-и:", missing_urls)

# Бачимо, що 4 посилання на товар не завантажилось (2 - помилка 403, 2 - посилання активне)